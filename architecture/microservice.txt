api design:
    in seperate doc which is seprated from code
    "user": {
        "description": "Represent a single user in the system",
        "fields": [
            {"name": "id", "type": "string"},
            {"name": "email", "type": "string", "required": false, "annotations": ["personal_data"]},
            {"name": "name", "type": "name", "annotations": ["personal_data"]},
            {"name": "status", "type": "user_status", "default": "active"},
        ]
    }

    //which is used to create user
    "user_from": {
    "fields": [
            {"name": "email", "type": "string", "required": false, "annotations": ["personal_data"]},
            {"name": "password", "type": "string", "required": false, "annotations": ["personal_data"]},
            {"name": "name", "type": "name_form", "required": false, "annotations": ["personal_data"]},
        ]
    }

    it should be consistent in the scope of entire company (validate use linter)
    automatic test to make sure the document is confirmed with the service
    code generation tool to generate service/http routing function for the framework


database architecture: (core: embrace eventual consistency)
    echo micro service application own its database
        not only about dependency 
        same data may save in different db in different format for different reason: performance, ..
        but for same info, there should has only one auth, which create and update the data
    no other service is allowed to connect to the database
    other service use only the service interface (API + Events) 

    tool to generate the db, table from the resource definition
    (add hash_code to object to check whether it has changed to avoid unnecessary io operation)


continuous delivery is very important
    deploy trigger by a git change(commit or tag) --- delta

standard health check
    "models": {
        "healthcheck": {
            "fields": [
                {"name" : "status", "type": "string", "example": "heathy"}
            ]
        }
    }

    "resources": {
        "healthcheck": {
            "path": "/_internal_",
            "operations": [
            {
                "method": "GET",
                "path": "/healthcheck",
                "response": {
                    "200": {"type": "healthcheck"},
                    "422": {"type": "io.flow.error.v0.models.generic_error"}
                }
            ]
        }
    }


we have an amazing api, but please subscribe to our evnet streams instead
event interface:
    first class schema for all events
    "unions": {
        "user_event": {
            "discriminator": "discriminator",
            "types": [
                {"type": "user_upserted"},
                {"type": "user_deleted"}
            ]
        }
    }

    "models": {
        "user_upserted": {
            "fields": [
                {"name": "event_id", "type": "string"},
                {"name": "timestamp", "type": "date-time-iso8601"},
                {"name": "user", "type": "io.flow.common.v0.models.user"}
            ]
        },

        "user_deleted":  {
            "fields": [
                {"name": "event_id", "type": "string"},
                {"name": "timestamp", "type": "date-time-iso8601"},
                {"name": "user", "type": "io.flow.common.v0.models.user"}
            ]
        }
    }
    producer guarantee at least once delivery
    consumers implememt idempotency

producer:
    create a journal of All operations on table (journal like event source)
    record operation (insert, update, delete)
    on creation, queue the journal record to be published
    real time, async, publish 1 event per journal record
    enable replay by simply queueing journal record

consumer:
    store new events in local database, partitioned for fast removal
    on event arrival, queue record to be consumed
    process incoming event in micro batch(by default every 250ms)
    record failure locally(report to center monitor system)

stream:
    has lib to handle stream
    stream has name which also should include the serialization method (json, grpc, etc)
    stream lib should easy for local end to end test (in memory queue)


dependencies:
    upgrade all servies every week to latest dependecies
    dependencies tracking



go-lang:
    https://github.com/goadesign/goa.git
    https://github.com/go-kit/kit


core axioms
    no components are privileged
    all components communicate in the same simple, homogeneous way
    components can be composed from other components

service communication
    rpc                    -->    command
    message queue          -->    event
service dependency (service <-> class)
    dependency tree
    depth of tree affect latency, set performance target
    merge service

data consistent

service mesh
    network of service proxy
    proxy ---> routing
               load balanceing
               health check
               retry + meltdown mechanism
               security

microservice arch expose much more bigger/widder attack interface 

Different performance, data-safety, security and scaling requirement for diffrerent microservice

microservice allow you to separate business-logic code from infrastructure code.
infrastructure code --> scaling, self healing, monitoring ...



functional breakdown
services and inter communication/message flow

message:
    synchronous  -- command
    asynchronous -- event
    oberve       -- others can see it
    consume      -- others cann't see it

asynchronous message:
    make interface idempotent
    message queue should support deliver at-least-once behavior
    message should contain metadata to identify

data
    data is heterogeneous not homogeneous -- relational db, document db, key-value db
    CQRS(command query responsibility segregation) 
        synchronous read and asynchronous write
        performance and eventual consistency.
    UUID is a default choice for primary key 
    data will be duplicated in different serivce and in this case, eventually consistency is preferred.



CD -- the ability to safely deploy a component(service) to production at any time
essential pipeline:
    version-controlled local development environment for each service, supported by unit test
    staging env to both validate the micorservice and build, reproducibly, and arifact for deployment


migration: //choose not vital/critical feature
    freedom and responsibility culture


product mgr -> ux -> dev -> QA -> DBA      sys adm -> net adm -> san adm
<-----microservice team ------------>      <---- platform team --------->


reduce production bug:
amount of changes that we have between each release of software is called batch size.
priciple:
    reduce batch size to the minimum allowable size we can handle

we need to be this tall to use microservices
    distribute system are difficult
    tracing, monitoring, log aggreation, resilience.
    network is unstable, bandwidth is limited.

patterns:
    event source
    materialized views
    command-query responsibility segregation(CQRS)

information collection:
1 the dependency structure within and outside the monolith
2 how a single request walks through the code base

migration:
    db table -> service -> provide api

decouple ui from app  ---> traffic routing
create new service:
    API design and boudnary of new service
    rewritten or copy the code from old monolith
    //think about the event which will be emit or consume by service
    new service connect directly to old db //with new db?
    add gateway between ui and monolith
    change monolith code from v1 to v2 which will use new service, and route part traffic to the v2
    remove v1 version of monolith
    one time ETL for new service, and new service will handle all traffic
    

use service mesh to route traffic based on http header
zero time deployment
blue/green: blue == current version, green == new version 
    only one accept all the traffic, if green has problem, roll back to blue immediately
    green relase should backward compatible

canary:
    redirect the amount of traffic to new version gradually.

A/B testing:
    two different and separate production environments to test a business hypothesis.
    there is no relation of current/new version, both version can be different branches of code.
    two version run simutanously to determine which one perfroms better in terms of business value
    it imply there is a very advanced monitoring system.


database migration (schama evoluation):
    part of software deployment process
    it's code, versioned with application code
    schema between consecutive release must be mutually compatible, we can't afford to lose any data.

CRUD:some data model for both read and write
CQRS:use different models to represent read and write operations. It's quite often that read and 
     write use different data store. Write operation will emit event which will be used to update
     read data store.
Event soursing:
    not only store the current state.
    trait the system state as a sequence of event. 
    save the events in a append-only data store.
    could use a store to save the current state but it's not the canonical source.

service dependency:
    the api of other service
    the event emit by other service
