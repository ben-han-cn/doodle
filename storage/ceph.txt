ceph /sef/ 

unified storage:
block device
filesystem
object

rdb -- kernel module to support ceph block device

user space distributed application
based on hosts filesystem
provide several interface
  lib based object storage
  adapter for vm as a block device
  http server with REST interface

metadata node + monitor node + osd(object storage device) node
monitor node has the cluster info
osd handle the data replication and management
metadata node has the attribute for objects including file system info
(block and object store doesn't need metadata daemon)

object look up is done by library(CRUSH algorithm: controlled replication under scalable hash)
mintor use paxo as consensus algorithm

node failure
osd has heartbeat each other
if one fails, its peers tell the monitor
if new osd turns on, it tells the monitor
the monitor issues a new OSD map recording the change in state
the osd cooperatively work together to move data to new nodes

object -> location(OSD id)
hash(obj name/key) % PG
CRUSH(pg, cluster state, rule set) //the latter two parameter is get from monitor


RADOS(Reliable autonomic distributed object store)

RBD (block device)
RADOS GW (RestAPI)   --> librados  --> RADOS --> MON + MDS + OSD
CephFS (file system)


read/write:
client --> one monitor node -> get five maps: monitor, OSD, MDS, CRUSH and PG map
data --> OSD id --> OSD daemon


PG(placement group) is a logical collection of objects that are replicated on OSD,
it's a logic container holding multiple objects, such that the logic container is 
mapped to multiple OSDs.

PG --> a acting set( a set of OSDs, first OSD is the primary)

pool is logic partition, pool includes PG, PG has object, PG is the basic replication unit.
which will be saved in different OSDs.



ceph rook

CRD:
    CephCluster
        cephVersion
        mon (counter)
        network (hostNetwork)
        rdbMirroring (workers)
        storage (nodes-name-path)

    CephFilesystem
        metadataPool
        dataPool
        metadataServer

Pods:
belongs to rook
    rook-ceph-agent (all worker nodes)
    rook-discover (all worker nodes)
    rook-ceph-operator (one)

belongs to csi
    csi-cephfsplugin (node controller in each worker node except master)
    csi-cephfsplugin-attacher (external attacher)
    csi-cephfsplugin-provisioner (external provisioner, has controller container inside)
    
belongs to ceph
    rook-ceph-mgr
    rook-ceph-mds
    rook-ceph-mon
    rook-ceph-osd
    rook-ceph-tools


implementation for csi:
    operator:
        start agent daemonset (flex related)
        start discover (update cm for device, support hotplug)
        StartCSIDrivers (start csi related pods)
        o.clusterController.StartWatch(namespaceToWatch, stopChan)
            start rook component
            start controller for CRDs (pool, object store, object store user, file system, nfs 
            start mon, osd health checker, start ceph status check
            addFinalizer to cluster obj


    file system controller:
