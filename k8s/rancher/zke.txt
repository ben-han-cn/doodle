ParseConfig
    host --> name, ip, user, role(controle, worker, etcd) 
    
TunnelHosts
    ssh then connect to docker daemon

GetClusterState
    use kubeclient to get configmap in k8s which save the cluster state

SetUpAuthentication
    caCrt, caKey, err := generateCACertAndKey()
    kubeAPICrt, kubeAPIKey, err := generateKubeAPICertAndKey(caCrt, caKey, kubeAPIAltNames)
    kubeControllerCrt, kubeControllerKey, err := generateClientCertAndKey(caCrt, caKey, KubeControllerCommonName, []string{})
    kubeSchedulerCrt, kubeSchedulerKey, err := generateClientCertAndKey(caCrt, caKey, KubeSchedulerCommonName, []string{})
    kubeProxyCrt, kubeProxyKey, err := generateClientCertAndKey(caCrt, caKey, KubeProxyCommonName, []string{})
    //kubelet == node
    nodeCrt, nodeKey, err := generateClientCertAndKey(caCrt, caKey, KubeNodeCommonName, []string{KubeNodeOrganizationName}
    kubeAdminCrt, kubeAdminKey, err := generateClientCertAndKey(caCrt, caKey, KubeAdminCommonName, []string{KubeAdminOrganizationName})
    --> map[string]CertificatePKI
    
SetUpHosts
    DeployCertificatesOnMasters
        CACertName
        KubeAPICertName
        KubeControllerName
        KubeSchedulerName
        KubeProxyName
        KubeNodeName
    DeployCertificatesOnWorkers
        CACertName
        KubeProxyName
        KubeNodeName
    DeployAdminConfig
        ioutil.WriteFile(KubeAdminConfigPath, []byte(kubeConfig), 0644)

DeployClusterPlanes
    services.RunEtcdPlane
        for _, host := range etcdHosts {
        imageCfg, hostCfg := buildEtcdConfig(host, etcdService)
        err := docker.DoRunContainer(host.DClient, imageCfg, hostCfg, EtcdContainerName, host.AdvertisedHostname, ETCDRole)
            if err != nil {
                return err 
            }   
        }   
    services.RunControlPlane
        for _, host := range controlHosts {
            err := runKubeAPI(host, etcdHosts, controlServices.KubeAPI)
            err = runKubeController(host, controlServices.KubeController)
            err = runScheduler(host, controlServices.Scheduler)
        }
    services.RunWorkerPlane
        for _, host := range controlHosts {
            err := runKubelet(host, workerServices.Kubelet, true)
            err = runKubeproxy(host, workerServices.Kubeproxy)
        }   
        for _, host := range workerHosts {
            err := runKubelet(host, workerServices.Kubelet, false)
            err = runKubeproxy(host, workerServices.Kubeproxy)
        }   

SaveClusterState
    save state to config map
DeployNetworkPlugin
    kubectl apply -f /network/plugin.yaml
DeployK8sAddOns
    kubectl apply -f /addons/kubedns*.yaml"


type Host struct {
    name 
    ip
    user
    role
}

type Cluster struct {
    engineConfig        ---> configure for each services
    etcdHost            []Host
    workerHost          []Host
    controlePlaneHost   []Host

    k8sServiceIP        net.IP
    certificates        map[string]pki.CertificatePKI
    
    clusterDomain       string
    clusterCIDR         string
    clusterDNSServer    string
}


service parameters:
etcd parameters
    "--name=etcd-" + host.AdvertisedHostname,
    "--data-dir=/etcd-data",
    "--advertise-client-urls=http://" + host.AdvertiseAddress + ":2379,http://" + host.AdvertiseAddress + ":4001",
    "--listen-client-urls=http://0.0.0.0:2379",
    "--initial-advertise-peer-urls=http://" + host.AdvertiseAddress + ":2380",
    "--listen-peer-urls=http://0.0.0.0:2380",
    "--initial-cluster-token=etcd-cluster-1",
    "--initial-cluster=etcd-" + host.AdvertisedHostname + "=http://" + host.AdvertiseAddress + ":2380"},


apiserver parameters
    "--admission-control=ServiceAccount,NamespaceLifecycle,LimitRanger,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds",
    "--runtime-config=batch/v2alpha1",
    "--runtime-config=authentication.k8s.io/v1beta1=true",
    "--storage-backend=etcd3",
    "--etcd-servers=" + etcdConnString,
    "--advertise-address=" + host.AdvertiseAddress,
    "--client-ca-file=" + pki.CACertPath,
    "--tls-cert-file=" + pki.KubeAPICertPath,
    "--tls-private-key-file=" + pki.KubeAPIKeyPath,
    "--service-account-key-file=" + pki.KubeAPIKeyPath


kube-controller-manager
    "--address=0.0.0.0",
    "--cloud-provider=",
    "--kubeconfig=/etc/kubernetes/ssl/kubecfg-controller-manager.yaml",
    "--enable-hostpath-provisioner=false",
    "--node-monitor-grace-period=40s",
    "--pod-eviction-timeout=5m0s",
    "--v=2",
    "--allocate-node-cidrs=true",
    "--cluster-cidr=" + kubeControllerService.ClusterCIDR,
    "--service-cluster-ip-range=" + kubeControllerService.ServiceClusterIPRange,
    "--service-account-private-key-file=" + pki.KubeAPIKeyPath,
    "--root-ca-file=" + pki.CACertPath,

kube-scheduler:
    --v=2",
    "--address=0.0.0.0",
    "--kubeconfig=/etc/kubernetes/ssl/kubecfg-scheduler.yaml"


kubeproxy:
    "--v=2",
    "--healthz-bind-address=0.0.0.0",
    "--kubeconfig=/etc/kubernetes/ssl/kubecfg-kube-proxy.yaml"


kubelet:
    "--v=2",
    "--address=0.0.0.0",
    "--cluster-domain=" + kubeletService.ClusterDomain,
    "--hostname-override=" + host.AdvertisedHostname,
    "--pod-infra-container-image=" + kubeletService.InfraContainerImage,
    "--cgroup-driver=cgroupfs",
    "--cgroups-per-qos=True",
    "--enforce-node-allocatable=",
    "--cluster-dns=" + kubeletService.ClusterDNSServer,
    "--network-plugin=cni",
    "--cni-conf-dir=/etc/cni/net.d",
    "--cni-bin-dir=/opt/cni/bin",
    "--resolv-conf=/etc/resolv.conf",
    "--allow-privileged=true",
    "--cloud-provider=",
    "--kubeconfig=" + pki.KubeNodeConfigPath,
    "--require-kubeconfig=True",

    if isMaster {
        imageCfg.Cmd = append(imageCfg.Cmd, "--register-with-taints=node-role.kubernetes.io/master=:NoSchedule")
        imageCfg.Cmd = append(imageCfg.Cmd, "--node-labels=node-role.kubernetes.io/master=true")
    }   

    hostCfg := &container.HostConfig{
        Binds: []string{
            "/etc/kubernetes:/etc/kubernetes",
            "/etc/cni:/etc/cni:ro",
            "/opt/cni:/opt/cni:ro",
            "/etc/resolv.conf:/etc/resolv.conf",
            "/sys:/sys:ro",
            "/var/lib/docker:/var/lib/docker:rw",
            "/var/lib/kubelet:/var/lib/kubelet:shared",
            "/var/run:/var/run:rw",
            "/run:/run",
            "/dev:/host/dev"},
        NetworkMode:   "host",
        PidMode:       "host",
        Privileged:    true,
        RestartPolicy: container.RestartPolicy{Name: "always"},
        PortBindings: nat.PortMap{
            "8080/tcp": []nat.PortBinding{
                {
                    HostIP:   "0.0.0.0",
                    HostPort: "8080",
                },
            },
        },
    }



//github.com/rancher/types/apis/management.cattle.io/v3/k8s_defaults.go 
//rke version to system service docker images mapping: 
//in Cluset.setClusterImageDefaults() ---> set the system images
K8sVersionToRKESystemImages = map[string]RKESystemImages{
    "v1.10.5-rancher1-1": {
        Etcd:                      m("quay.io/coreos/etcd:v3.1.12"),
        Kubernetes:                m("rancher/hyperkube:v1.10.5-rancher1"),
        Alpine:                    m("rancher/rke-tools:v0.1.10"),
        NginxProxy:                m("rancher/rke-tools:v0.1.10"),
        CertDownloader:            m("rancher/rke-tools:v0.1.10"),
        KubernetesServicesSidecar: m("rancher/rke-tools:v0.1.10"),
        KubeDNS:                   m("gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.8"),
        DNSmasq:                   m("gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.8"),
        KubeDNSSidecar:            m("gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.8"),
        KubeDNSAutoscaler:         m("gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.0.0"),        
        Flannel:                   m("quay.io/coreos/flannel:v0.9.1"),
        FlannelCNI:                m("quay.io/coreos/flannel-cni:v0.2.0"),
        CalicoNode:                m("quay.io/calico/node:v3.1.1"),
        CalicoCNI:                 m("quay.io/calico/cni:v3.1.1"),
        CalicoCtl:                 m("quay.io/calico/ctl:v2.0.0"),
        CanalNode:                 m("quay.io/calico/node:v3.1.1"),
        CanalCNI:                  m("quay.io/calico/cni:v3.1.1"),
        CanalFlannel:              m("quay.io/coreos/flannel:v0.9.1"),
        WeaveNode:                 m("weaveworks/weave-kube:2.1.2"),
        WeaveCNI:                  m("weaveworks/weave-npc:2.1.2"),
        PodInfraContainer:         m("gcr.io/google_containers/pause-amd64:3.1"),
        Ingress:                   m("rancher/nginx-ingress-controller:0.10.2-rancher3"),
        IngressBackend:            m("k8s.gcr.io/defaultbackend:1.4"),
    },
}




zke failure:
    docker unhealty, volume is too big
    time isn't synced between master and worker
