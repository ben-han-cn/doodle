service == a group of pods with a static ip:port
which make pods could be dynamic scheduled

so a application should have one or several service.

traffic route to service is load balance to the pods it contains.

apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  sessionAffinity: ClientIP //configure session affinity otherwise underlaying pods will be
                            //randomly selected
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
  selector:
    app: kubia 


service's ip address is cluster address, which means its primary purpose is
exposing a group of pods to other pods in the cluster.

run curl command on pod kubia-7nog1
$ kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153
-- sgnals the end of command options for kubectl. otherwise s will be interpreted 
as option for kubectl.

service discovery
1 environment variable
when new pod is created, k8s initialize a set of the environment variables pointing to
each service that exists at that moment
$ kubectl exec kubia-3inly env
KUBIA_SERVICE_HOST=10.111.249.153
KUBIA_SERVICE_PORT=80 
....

2 dns
/etc/resolv.conf
nameserver ---> default pod
search default.svc.cluster.local svc.cluster.local cluster.local

backend-database.default.svc.cluster.local
service_name + namespace + svc.cluster.local


service includes a list of endpoints
$ kubectl get endpoints kubia
NAME  ENDPOINTS                                         AGE
kubia 10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   1h



service point to outside server
1 define service
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  ports:
  - port: 80 

2 define endpoints
apiVersion: v1
kind: Endpoints
metadata:
  name: external-service
subsets:
  - addresses:
    - ip: 11.11.11.11
    - ip: 22.22.22.22
    ports:
    - port: 80

since service has no pod selector, so endpoints should be created
manually, and endpoint name must match service name.


3 extern service alias
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  type: ExternalName
  externalName: someapi.somecompany.com
  ports:
  - port: 80
ExternalName will create a cname, which just hide the extern service name
from cluster pods

4 expose to extern client
4.1 nodeport service
apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30123
  selector:
    app: kubia

$ kubectl get svc kubia-nodeport
NAME           CLUSTER-IP     EXTERNAL-IP  PORT(S)        AGE
kubia-nodeport 10.111.254.223 <nodes>      80:30123/TCP   2m
the service could be accessed by
10.111.254.223:80
<node1>ip:30123
<node2>ip:30123
....

4.2 external load balancer
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
  app: kubia

$ kubectl get svc kubia-loadbalancer
NAME                CLUSTER-IP      EXTERNAL-IP     PORT(S)       AGE
kubia-loadbalancer  10.111.241.153  130.211.53.173  80:32143/TCP  1m

a loadbalancer type service is a nodeport service with an additional 
infrastructure-provided load balancer.

4.3 ingress
http level dispatch, the target is a service object.
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /kubia
        backend:
          serviceName: kubia
          servicePort: 80
      - path: /foo
        backend:
          serviceName: bar
          servicePort: 80 

  - host: bar.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: bar
          servicePort: 80


5 readiness probe
pod in service when failed in readiness probe, it will remove from the service, and
when succeed, it will be re-added into the service. Different from liveness probe, failed 
container won't be killed or restarted.

apiVersion: v1
kind: ReplicationController
...
spec:
  spec:
    containers:
    - name: kubia
      image: luksa/kubia
      readinessProbe:
        exec:
          command:
          - ls
          - /var/ready 
        initialDelaySeconds: 15
        timeoutSeconds: 1
        periodSeconds: 15

1 Readiness probe fails.
2 Kubernetes stops routing traffic to the pod.
3 Liveness probe fails.
4 Kubernetes restarts the failed container*.
5 Readiness probe succeeds.
6 Kubernetes starts routing traffic to the pod again.


6 headless service
make the clusterIP in spec of service to None, make it a headless, in this case, clustip won't
be assigned to the service. In this case, the service name dns request will return all the pods
ips, and load balancing mechanism is implemented by dns. 
