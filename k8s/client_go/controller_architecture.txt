The controller:
    has a reference to the DeltaFIFO queue;
    has a reference to the downstream store;
    has a reference to the ListerWatcher (the upstream source in our case);
    has a process loop, 
        get delta form DeltaFIFO and put it to downstream store
        call the hook in event handler
    creates a Reflector.

The reflector:
    has a reference to the same DeltaFIFO queue (called store internally);
    has a reference to the same ListerWatcher;
    lists and watches the ListerWatcher;
    is responsible for producing the FIFO queueâ€™s input;
    is responsible for calling the Resync method on the FIFO queue every resyncPeriod ns.

The DeltaFIFO queue:
    has a reference to the downstream store;
    has a queue of Deltas for objects that were listed and watched by the Reflector.
        Deltas = []Delta = [](option[add, delete, update, sync], object)

    type DeltaFIFO struct {
        items map[string]Deltas
        queue []string
        keyFunc KeyFunc
        knownObjects KeyListerGetter
    }
    queue is keep the event order for each object
    Deltas keep the event order for one object
    design issue:
        from the code, deltafifo needs KeyListerGetter to get the latest event
        but, DeltaFIFO dosen't has any logic to sync state with the underlay store
        it's the controller who calls Pop function of DeltaFIFO to do the synchronization.
        
    
how reflector fill DeltaFIFO:
1 get the initial state, use ResourceVersion = "0" to get all the resources
    func (f *DeltaFIFO) Replace(list []interface{}, resourceVersion string) error  {
            for _, item := range list {
                key, err := f.KeyOf(item)
                f.queueActionLocked(Sync, item) //put Sync Delta into the queue
            }

            //delete any unknwon/old object
            for _, k := range knownKeys {
                if keys.Has(k) {
                    continue
                }       
                f.queueActionLocked(Deleted, DeletedFinalStateUnknown{k, deletedObj})
            }       

            //record the initial state
            if !f.populated {
                f.populated = true
                f.initialPopulationCount = len(list) + queuedDeletions
            }
    }

    //HasSynced == all the initial list objects have been processed
    //which means downstream store has get a snapshot of cared objects
    func (f *DeltaFIFO) HasSynced() bool {
        f.lock.Lock()
        defer f.lock.Unlock()
        return f.populated && f.initialPopulationCount == 0
    }

2 feed the delta to DeltaFIFO:
    reflector: watchHandler
    deltafifo.Add()     --> f.queueActionLocked(Added, obj)
    deltafifo.Update()  --> f.queueActionLocked(Updated, obj)
    deltafifo.Delete()  --> f.queueActionLocked(Deleted, obj) //existance of obj is checked
    func (f *DeltaFIFO) queueActionLocked(actionType DeltaType, obj interface{}) error {
        id, err := f.KeyOf(obj)

        //Sync is normally handled as add, so if last delata is delete, no sync should be pushed
        if actionType == Sync && f.willObjectBeDeletedLocked(id) {
            return nil
        }

        newDeltas := append(f.items[id], Delta{actionType, obj})
        newDeltas = dedupDeltas(newDeltas) //remove duplicate event, continuous delete is duplicate
        _, exists := f.items[id]
        
        if len(newDeltas) > 0 {
            if !exists {
                f.queue = append(f.queue, id)
            }
            f.items[id] = newDeltas
            f.cond.Broadcast()
        } else if exists {      //this branch should never be reached
            delete(f.items, id)
        }
        return nil
    }
    
3 periodically resync the DeltaFIFO:
    func (f *DeltaFIFO) Resync() error {
        keys := f.knownObjects.ListKeys()
        for _, k := range keys {
            f.syncKeyLocked(k)
        }
    }
    
    func (f *DeltaFIFO) syncKeyLocked(key string) error {
        obj, exists, err := f.knownObjects.GetByKey(key)
        id, err := f.KeyOf(obj)
        //there are pending events, so no need resync
        if len(f.items[id]) > 0 {
            return nil
        }
        f.queueActionLocked(Sync, obj)
    }

    resync just let handler(in controller) to get the chance to walk through
    the whole objects in downstream store, normally, OnUpdate will be called


client-go usage:
1 get client --- kubernetes.ClientSet ---- kubernetes.NewForConfig(config)
2 get basic CRUDs
  nodes, err := client.Core().Nodes().List(v1.ListOptions{FieldSelector: "xxx"})
  node := nodes.Items[0]
  node.Annotations["checked"] = true
  updatedNode, err := client.Core().Nodes().Update(&node)
  gracePeriod := int64(10)
  err = client.Core().Nodes().Delete(updateNode.Name, &v1.DeleteOptions{GracePeriodSeconds: &gracePeriod})
3 watch updates
  watchList := cache.NewListWatchFromClient(client.Core().RESTClient(), "nodes", v1.NamesapceAll, fields.Everything())
  store, controller := cache.NewInformer(
    watchList,
    &api.Node{},
    time.Seconds*30,
    cache.ResourceEventHandlerFuncs{
      AddFunc: handleNodeAdd,
      UpdateFunc: handleNodeUpdate,
    },
  )
  stop := make(chan struct{})
  go controller.Run(stop)
  nodeInterface, exists, err := store.GetByKey("minikube")
  ....

  //shared informer which could have several controller
  informer := cache.NewSharedIndexInformer(
    watchList,
    &api.Node{},
    time.Second*10,
    cache.Indexers{},
  )

  informer.AddEventHandler(cache.ResourceEventHandlerFuncs {
    AddFunc:    handleNodeAdd,
    UpdateFunc: handleNodeUpdate,
  })
  informer.AddEventHandler(cache.ResourceEventHandlerFuncs {
    AddFunc:    handleNodeAddExtra,
    UpdateFunc: handleNodeUpdateExtra,
  })

controller pattern

           +----------------+
           |                |                     CRUD
           | api server     |<----------------------------------------------+
           |                |                                               |
           +-------+--------+                                               |
         List/watch|                                                        |
                   |                                                        |
           Informer|                                                  +-----+----+
           +-------|--------+                                         |client    |
           |       |        |                                         +-----^----+
           |   +---v----+   |                                               |
           |   |        |   |                                               |
           |   |reflector   |                                               |
           |   |        |   |                                               |
           |   +---+----+   |      +---------+                        +-----+---+
           |       |        |      |         |                        | worker  |
           |   +---v----+   |      |callbacks|                   +---->         |
           |   |        +--------->|         |      +------------|    +---------+
           |   |deltafifo   |      |OnAdd    |      |            |    +---------+
           |   |        |   |      |OnUpdate +----->|workqueue   +---->         |
           |   +---+----+   |      |OnDelete |      +------------|    | worker  |
           |       |        |      |         |                   |    +---------+
           |   +---v----+   |      +---------+                   +---->---------+
           |   |        |   |                    Readonly             |         |
           |   |store   +--------------------------------------------->worker   |
           |   +--------+   |                                         +---------+
           +----------------+


generator:
client-gen
conversion-gen
deepcopy-gen
defaulter-gen
go-to-protobuf
informer-gen
lister-gen
openapi-gen
codec-gen


1 API aggregation provides far greater flexibility
2 Generate supporting code, don't write it yourself
3 follow the types.go rules, Generators are picky
4 never modify resources in the cache
5 define conversions between external versions


queue is useful in following scenario:
    rate limiting
    collapse multiple object updates into a single one in case update is too frequent
    wait for cache synced before processing events
